{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Libraries for the Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "#Beth Computer Location File\n",
    "#trainData = pd.read_csv(\"/Users/bethanydanner/Google_Drive/documents/python_code/clustering-based-anomaly-detection/Dataset/NSL-KDD/KDDTrain+.csv\", header = None)\n",
    "#testData = pd.read_csv(\"/Users/bethanydanner/Google_Drive/documents/python_code/clustering-based-anomaly-detection/Dataset/NSL-KDD/KDDTest+.csv\", header = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the  Train Dataset and Checking if has missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.0\n",
       "1     0.0\n",
       "2     0.0\n",
       "3     0.0\n",
       "4     0.0\n",
       "5     0.0\n",
       "6     0.0\n",
       "7     0.0\n",
       "8     0.0\n",
       "9     0.0\n",
       "10    0.0\n",
       "11    0.0\n",
       "12    0.0\n",
       "13    0.0\n",
       "14    0.0\n",
       "15    0.0\n",
       "16    0.0\n",
       "17    0.0\n",
       "18    0.0\n",
       "19    0.0\n",
       "20    0.0\n",
       "21    0.0\n",
       "22    0.0\n",
       "23    0.0\n",
       "24    0.0\n",
       "25    0.0\n",
       "26    0.0\n",
       "27    0.0\n",
       "28    0.0\n",
       "29    0.0\n",
       "30    0.0\n",
       "31    0.0\n",
       "32    0.0\n",
       "33    0.0\n",
       "34    0.0\n",
       "35    0.0\n",
       "36    0.0\n",
       "37    0.0\n",
       "38    0.0\n",
       "39    0.0\n",
       "40    0.0\n",
       "41    0.0\n",
       "42    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData = pd.read_csv(\"/Users/jeremyperez/Jupyter/NSL-KDD/KDDTrain+.csv\", header = None) \n",
    "#Run a Missing Value Ratio test to determine if any feature is missing values.\n",
    "#If all ratios = 0.0, then data is not missing any values for any features.\n",
    "\n",
    "#More info about Missing value ratio at \n",
    "#https://www.analyticsvidhya.com/blog/2018/08/dimensionality-reduction-techniques-python/\n",
    "\n",
    "trainData.isnull().sum()/len(trainData)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the  Test Dataset and Checking if has missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.004436\n",
       "1     0.000000\n",
       "2     0.000000\n",
       "3     0.000000\n",
       "4     0.004436\n",
       "5     0.004436\n",
       "6     0.004436\n",
       "7     0.004436\n",
       "8     0.004436\n",
       "9     0.004436\n",
       "10    0.004436\n",
       "11    0.004436\n",
       "12    0.004436\n",
       "13    0.004436\n",
       "14    0.004436\n",
       "15    0.004436\n",
       "16    0.004436\n",
       "17    0.004436\n",
       "18    0.004436\n",
       "19    0.004436\n",
       "20    0.004436\n",
       "21    0.004436\n",
       "22    0.004436\n",
       "23    0.004436\n",
       "24    0.004436\n",
       "25    0.004436\n",
       "26    0.004436\n",
       "27    0.004436\n",
       "28    0.004436\n",
       "29    0.004436\n",
       "30    0.004436\n",
       "31    0.004436\n",
       "32    0.004436\n",
       "33    0.004436\n",
       "34    0.004436\n",
       "35    0.004436\n",
       "36    0.004436\n",
       "37    0.004436\n",
       "38    0.004436\n",
       "39    0.004436\n",
       "40    0.004436\n",
       "41    0.004436\n",
       "42    0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData = pd.read_csv(\"/Users/jeremyperez/Jupyter/NSL-KDD/KDDTest+.csv\", header = None)\n",
    "#Run a Missing Value Ratio test to determine if any feature is missing values.\n",
    "#If all ratios = 0.0, then data is not missing any values for any features.\n",
    "testData.isnull().sum()/len(testData)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the Dependent and independent Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = trainData.iloc[:,:-1].values # Get all the rows and all the clums except all the colums - 1\n",
    "Y = trainData.iloc[:,42].values# F- Score & Get all the rows and the colum number 42\n",
    "A = testData.iloc[:,:-1].values # Get all the rows and all the clums except all the colums - 1\n",
    "Z = testData.iloc[:,42].values# Get all the rows and the colum number 42\n",
    "\n",
    "###############################################################################################\n",
    "#removing Categorical data from the data set\n",
    "noCatg = trainData.iloc[:,[0,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41]].values\n",
    "attacks = trainData.iloc[:,42].values #Attacks with no one hot encoding For k-means and dbscan crosstab \n",
    "###############################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Categorical Data for Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "#We use One hot encoding to pervent the machine learning to atribute the categorical data in order. \n",
    "#What one hot encoding(ColumnTransformer) does is, it takes a column which has categorical data, \n",
    "#which has been label encoded, and then splits the column into multiple columns.\n",
    "#The numbers are replaced by 1s and 0s, depending on which column has what value\n",
    "#We don't need to do a label encoded step because ColumnTransformer do one hot encode and label encode!\n",
    "\n",
    "###############################################################################################################\n",
    "#Manually encode\n",
    "#protocols = {'tcp': 0,'udp': 1,'icmp': 2} \n",
    "#servers  = {'http': 0, 'domain_u': 1, 'sunrpc': 2, 'smtp': 3, 'ecr_i': 4, 'iso_tsap': 5, 'private': 6, 'finger': 7, 'ftp': 8, 'telnet': 9,'other': 10,'discard': 11, 'courier': 12, 'pop_3': 13, 'ldap': 14, 'eco_i': 15, 'ftp_data': 16, 'klogin': 17, 'auth': 18, 'mtp': 19, 'name': 20, 'netbios_ns': 21,'remote_job': 22,'supdup': 23,'uucp_path': 24,'Z39_50': 25,'csnet_ns': 26,'uucp': 27,'netbios_dgm': 28,'urp_i': 29,'domain': 30,'bgp':31,'gopher': 32,'vmnet': 33,'systat': 34,'http_443': 35,'efs': 36,'whois': 37,'imap4': 38,'echo': 39,'link': 40,'login': 41,'kshell': 42,'sql_net': 43,'time': 44,'hostnames': 45,'exec': 46,'ntp_u': 47,'nntp': 48,'ctf': 49,'ssh': 50,'daytime': 51,'shell': 52,'netstat': 53,'nnsp': 54,'IRC': 55,'pop_2': 56,'printer': 57,'tim_i': 58,'pm_dump': 59,'red_i': 60,'netbios_ssn': 61,'rje': 62,'X11': 63,'urh_i': 64,'http_8001': 65,'aol': 66,'http_2784': 67,'tftp_u': 68,'harvest': 69} \n",
    "#servers_error  = {'REJ': 0, 'SF': 1, 'S0': 2, 'RSTR': 3, 'RSTO': 4,'SH': 5,'S1': 6,'RSTOS0': 7,'S3': 8,'S2': 9,'OTH': 10} \n",
    "#attacks  = {'normal': 0, 'neptune': 1, 'warezclient': 2, 'ipsweep': 3, 'mscan': 4, 'back': 5, 'smurf': 6, 'mailbomb': 7, 'apache2': 8, 'rootkit': 9,'back': 10,'satan': 11, 'processtable': 12, 'guess_passwd': 13, 'saint': 14,'portsweep': 15,'teardrop': 16,'nmap': 17,'pod': 18,'ftp_write': 19,'multihop': 20,'buffer_overflow': 21,'imap': 22,'warezmaster': 21,'phf': 22,'land': 23,'loadmodule': 24,'spy': 25,'perl': 26,'snmpgetattack': 27,'httptunnel': 28,'ps': 29,'snmpguess': 30,'named': 31,'sendmail':32,'xterm':33,'worm': 34,'xlock': 35,'xsnoop': 36,'sqlattack': 37,'udpstorm':38} \n",
    "\n",
    "#Y[1] = [attacks[item] for item in Y[1]]\n",
    "#Y[2] = [servers[item] for item in Y[2]]\n",
    "#Y[3] = [servers_error[item] for item in Y[3]]\n",
    "#Y[42] = [attacks[item] for item in Y[42]]\n",
    "###############################################################################################################\n",
    "\n",
    "#One Hot Encoding to the attacks\n",
    "#Encoding the Independient Variable\n",
    "transformX = ColumnTransformer([(\"Servers\", OneHotEncoder(categories = \"auto\"), [1,2,3])], remainder=\"passthrough\")\n",
    "X = transformX.fit_transform(X)\n",
    "\n",
    "\n",
    "#Encoding the Dependent Variable \n",
    "Y = pd.DataFrame(Y)\n",
    "Y = np.array(Y)\n",
    "\n",
    "Z = pd.DataFrame(Z)\n",
    "Z = np.array(\n",
    "transformY= ColumnTransformer([(\"Attacks\", OneHotEncoder(categories = \"auto\"), [0])], remainder=\"passthrough\")\n",
    "Y = transformY.fit_transform(Y)\n",
    "Y = pd.DataFrame(Y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################################################\n",
    "#Label Encoding to the attacks\n",
    "\n",
    "#labelencoder_y = LabelEncoder()\n",
    "#Y = labelencoder_y.fit_transform(Y)\n",
    "\n",
    "labelencoder_A = LabelEncoder()\n",
    "Z = labelencoder_A.fit_transform(Z)\n",
    "#pd.get_dummies(,drop_first=True)\n",
    "###############################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Categorical Data for Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding Categorical Data for Test Set\n",
    "#Encoding the Independient Variable\n",
    "Z = pd.DataFrame(Z)\n",
    "Z = np.array(Z)\n",
    "transformA = ColumnTransformer([(\"Servers\", OneHotEncoder(), [1,2,3])], remainder=\"passthrough\")\n",
    "A = transformA.fit_transform(A)\n",
    "    \n",
    "#Encoding the Dependent Variable\n",
    "transformZ = ColumnTransformer([(\"Attacks\", OneHotEncoder(), [0])], remainder=\"passthrough\")\n",
    "Z = transformZ.fit_transform(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling the Train data with Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Because we are using numerical-value-only clustering techniques to analyze the NSL-KDD dataset,\n",
    "#we need to normalize the values in the dataset, as Ibrahim., et. al. describe (page 112).\n",
    "#We complete the normalization process below:\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "normalizer = Normalizer().fit(X)\n",
    "X = normalizer.transform(X)\n",
    "trainData = np.array(X)\n",
    "trainLabel = np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling the Test data with Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer().fit(A)\n",
    "A = normalizer.transform(A)\n",
    "testData =  np.array(A)\n",
    "testLabel = np.array(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regresion method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################################\n",
    "#model = LogisticRegression(solver = 'lbfgs')\n",
    "#model.fit(trainData,trainLabel)\n",
    "###############################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elbow Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elbow method to find the best number of culster\n",
    "from sklearn.cluster import KMeans\n",
    "wcss = []\n",
    "for i in range(1,11):\n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++',max_iter = 300,n_init = 10,random_state = 0)\n",
    "    kmeans.fit(trainData)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(1,11),wcss)\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying K-mea(n_clusters = 5)\n",
    "KMEANS = KMeans(n_clusters = 4, init = 'k-means++',max_iter = 300,n_init = 10,random_state = 0)\n",
    "kmeans = KMEANS.fit(trainData)\n",
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Results of K-means by Crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(attacks,kmeans.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Results by Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visual representation of the clusters\n",
    "#plt.scatter(X[y_kmeans ==0,0],X[y_kmeans == 0,1], s = 21, c = 'red', label = 'cluster1')\n",
    "#plt.scatter(X[y_kmeans ==1,0],X[y_kmeans == 1,1], s = 21, c = 'yellow', label = 'cluster2')\n",
    "#plt.scatter(X[y_kmeans ==2,0],X[y_kmeans == 2,1], s = 21, c = 'cyan', label = 'cluster3')\n",
    "#plt.scatter(X[y_kmeans ==3,0],X[y_kmeans == 3,1], s = 21, c = 'orange', label = 'cluster4')\n",
    "#plt.scatter(X[y_kmeans ==4,0],X[y_kmeans == 4,1], s = 21, c = 'black', label = 'cluster5')\n",
    "#plt.scatter(kmeans.cluster_centers_[:, 0],kmeans.cluster_centers_[:, 1],s = 300, c = 'purple', label = 'Centroids')\n",
    "#plt.title('Clusters of Attacks')\n",
    "#plt.xlabel('Numbers of Attacks')\n",
    "#plt.ylabel('Types of Attacks')\n",
    "#plt.legend()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import accuracy_score\n",
    "#OPTICS\n",
    "# #############################################################################\n",
    "# Compute DBSCAN\n",
    "db = DBSCAN(eps=0.3, min_samples=20000).fit(trainData)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Results of DBSCAN by Crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(attacks,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F-Score implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(Y,kmeans.labels_, average=\"weighted\") #[None, 'micro', 'macro', 'weighted']\n",
    "f1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "923px",
    "left": "328px",
    "right": "20px",
    "top": "9px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
