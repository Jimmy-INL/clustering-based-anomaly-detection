{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries for the Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from scipy.io import arff\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (precision_score, recall_score,f1_score, accuracy_score,mean_squared_error,mean_absolute_error)\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import (precision_score, recall_score,f1_score, accuracy_score,mean_squared_error,mean_absolute_error, roc_curve, classification_report,auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# creating file handler for  \n",
    "# our KDDTrain+.txt and KDDTest+.txt files in \n",
    "# read mode \n",
    "trainData_handler = open(\"/Users/jeremyperez/Jupyter/Dataset/KDDTrain+.csv\", \"r\")\n",
    "testData_handler = open(\"/Users/jeremyperez/Jupyter/Dataset/KDDTest+.csv\", \"r\")\n",
    "\n",
    "# creating a Pandas DataFrame \n",
    "# using read_csv function  \n",
    "# that reads from a csv file. \n",
    "trainData = pd.read_csv(trainData_handler, sep = \",\") \n",
    "testData = pd.read_csv(testData_handler, sep = \",\") \n",
    "  \n",
    "# closing the file handler \n",
    "trainData_handler.close() \n",
    "testData_handler.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting String Values  to Unique Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocol = {'tcp': 0,'udp': 1,'icmp': 2}\n",
    "service = {'http': 0, 'domain_u': 1, 'sunrpc': 2, 'smtp': 3, 'ecr_i': 4, 'iso_tsap': 5, 'private': 6, 'finger': 7, 'ftp': 8, 'telnet': 9,'other': 10,'discard': 11, 'courier': 12, 'pop_3': 13, 'ldap': 14, 'eco_i': 15, 'ftp_data': 16, 'klogin': 17, 'auth': 18, 'mtp': 19, 'name': 20, 'netbios_ns': 21,'remote_job': 22,'supdup': 23,'uucp_path': 24,'Z39_50': 25,'csnet_ns': 26,'uucp': 27,'netbios_dgm': 28,'urp_i': 29,'domain': 30,'bgp':31,'gopher': 32,'vmnet': 33,'systat': 34,'http_443': 35,'efs': 36,'whois': 37,'imap4': 38,'echo': 39,'link': 40,'login': 41,'kshell': 42,'sql_net': 43,'time': 44,'hostnames': 45,'exec': 46,'ntp_u': 47,'nntp': 48,'ctf': 49,'ssh': 50,'daytime': 51,'shell': 52,'netstat': 53,'nnsp': 54,'IRC': 55,'pop_2': 56,'printer': 57,'tim_i': 58,'pm_dump': 59,'red_i': 60,'netbios_ssn': 61,'rje': 62,'X11': 63,'urh_i': 64,'http_8001': 65,'aol': 66,'http_2784': 67,'tftp_u': 68,'harvest': 69}\n",
    "server_error = {'REJ': 0, 'SF': 1, 'S0': 2, 'RSTR': 3, 'RSTO': 4,'SH': 5,'S1': 6,'RSTOS0': 7,'S3': 8,'S2': 9,'OTH': 10}\n",
    "attacks= {'normal': 0, 'neptune': 1, 'warezclient': 2, 'ipsweep': 3, 'mscan': 4, 'back': 5, 'smurf': 6, 'mailbomb': 7, 'apache2': 8, 'rootkit': 9,'back': 10,'satan': 11, 'processtable': 12, 'guess_passwd': 13, 'saint': 14,'portsweep': 15,'teardrop': 16,'nmap': 17,'pod': 18,'ftp_write': 19,'multihop': 20,'buffer_overflow': 21,'imap': 22,'warezmaster': 21,'phf': 22,'land': 23,'loadmodule': 24,'spy': 25,'perl': 26,'snmpgetattack': 27,'httptunnel': 28,'ps': 29,'snmpguess': 30,'named': 31,'sendmail':32,'xterm':33,'worm': 34,'xlock': 35,'xsnoop': 36,'sqlattack': 37,'udpstorm':38}\n",
    "  \n",
    "# traversing through dataframe \n",
    "# Protocol,Service,Server error and Attacks column's and writing \n",
    "# values where key matches in the train dataset\n",
    "trainData.Protocol = [protocol[item] for item in trainData.Protocol]\n",
    "trainData.Service = [service[item] for item in trainData.Service]\n",
    "trainData.Server_error = [server_error[item] for item in trainData.Server_error]\n",
    "trainData.Attacks = [attacks[item] for item in trainData.Attacks]\n",
    "\n",
    "# traversing through dataframe \n",
    "# Protocol,Service,Server error and Attacks column's and writing \n",
    "# values where key matches in the test dataset\n",
    "testData.Protocol = [protocol[item] for item in testData.Protocol]\n",
    "testData.Service = [service[item] for item in testData.Service]\n",
    "testData.Server_error = [server_error[item] for item in testData.Server_error]\n",
    "testData.Attacks = [attacks[item] for item in testData.Attacks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/envs/Machinelearning/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/miniconda3/envs/Machinelearning/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X = trainData.iloc[:,1:42]\n",
    "Y = trainData.iloc[:,0]\n",
    "C = testData.iloc[:,0]\n",
    "T = testData.iloc[:,1:42]\n",
    "\n",
    "scaler = Normalizer().fit(X)\n",
    "trainX = scaler.transform(X)\n",
    "scaler = Normalizer().fit(T)\n",
    "testT = scaler.transform(T)\n",
    "\n",
    "traindata = np.array(trainX)\n",
    "trainlabel = np.array(Y)\n",
    "testdata = np.array(testT)\n",
    "testlabel = np.array(C)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(traindata, trainlabel)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(traindata, trainlabel)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "expected = testlabel\n",
    "predicted = model.predict(testdata)\n",
    "np.savetxt('res/predictedLR.txt', predicted, fmt='%01d')\n",
    "accuracy = accuracy_score(expected, predicted)\n",
    "recall = recall_score(expected, predicted, average=\"binary\")\n",
    "precision = precision_score(expected, predicted , average=\"binary\")\n",
    "f1 = f1_score(expected, predicted , average=\"binary\")\n",
    "cm = metrics.confusion_matrix(expected, predicted)\n",
    "print(cm)\n",
    "tpr = float(cm[0][0])/np.sum(cm[0])\n",
    "fpr = float(cm[1][1])/np.sum(cm[1])\n",
    "print(\"%.3f\" %tpr)\n",
    "print(\"%.3f\" %fpr)\n",
    "print(\"Accuracy\")\n",
    "print(\"%.3f\" %accuracy)\n",
    "print(\"precision\")\n",
    "print(\"%.3f\" %precision)\n",
    "print(\"recall\")\n",
    "print(\"%.3f\" %recall)\n",
    "print(\"f-score\")\n",
    "print(\"%.3f\" %f1)\n",
    "print(\"fpr\")\n",
    "print(\"%.3f\" %fpr)\n",
    "print(\"tpr\")\n",
    "print(\"%.3f\" %tpr)\n",
    "print(\"***************************************************************\")\n",
    "\n",
    "\n",
    "# fit a Naive Bayes model to the data\n",
    "model = GaussianNB()\n",
    "model.fit(traindata, trainlabel)\n",
    "print(model)\n",
    "# make predictions\n",
    "expected = testlabel\n",
    "predicted = model.predict(testdata)\n",
    "np.savetxt('res/predictedNB.txt', predicted, fmt='%01d')\n",
    "accuracy = accuracy_score(expected, predicted)\n",
    "recall = recall_score(expected, predicted, average=\"binary\")\n",
    "precision = precision_score(expected, predicted , average=\"binary\")\n",
    "f1 = f1_score(expected, predicted , average=\"binary\")\n",
    "cm = metrics.confusion_matrix(expected, predicted)\n",
    "print(cm)\n",
    "tpr = float(cm[0][0])/np.sum(cm[0])\n",
    "fpr = float(cm[1][1])/np.sum(cm[1])\n",
    "print(\"%.3f\" %tpr)\n",
    "print(\"%.3f\" %fpr)\n",
    "print(\"Accuracy\")\n",
    "print(\"%.3f\" %accuracy)\n",
    "print(\"precision\")\n",
    "print(\"%.3f\" %precision)\n",
    "print(\"recall\")\n",
    "print(\"%.3f\" %recall)\n",
    "print(\"f-score\")\n",
    "print(\"%.3f\" %f1)\n",
    "print(\"fpr\")\n",
    "print(\"%.3f\" %fpr)\n",
    "print(\"tpr\")\n",
    "print(\"%.3f\" %tpr)\n",
    "print(\"***************************************************************\")\n",
    "\n",
    "\n",
    "\n",
    "# fit a k-nearest neighbor model to the data\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(traindata, trainlabel)\n",
    "print(model)\n",
    "# make predictions\n",
    "expected = testlabel\n",
    "predicted = model.predict(testdata)\n",
    "np.savetxt('res/predictedKNN.txt', predicted, fmt='%01d')\n",
    "# summarize the fit of the model\n",
    "accuracy = accuracy_score(expected, predicted)\n",
    "recall = recall_score(expected, predicted, average=\"binary\")\n",
    "precision = precision_score(expected, predicted , average=\"binary\")\n",
    "f1 = f1_score(expected, predicted , average=\"binary\")\n",
    "\n",
    "cm = metrics.confusion_matrix(expected, predicted)\n",
    "print(cm)\n",
    "tpr = float(cm[0][0])/np.sum(cm[0])\n",
    "fpr = float(cm[1][1])/np.sum(cm[1])\n",
    "print(\"%.3f\" %tpr)\n",
    "print(\"%.3f\" %fpr)\n",
    "print(\"Accuracy\")\n",
    "print(\"%.3f\" %accuracy)\n",
    "print(\"precision\")\n",
    "print(\"%.3f\" %precision)\n",
    "print(\"recall\")\n",
    "print(\"%.3f\" %recall)\n",
    "print(\"f-score\")\n",
    "print(\"%.3f\" %f1)\n",
    "print(\"fpr\")\n",
    "print(\"%.3f\" %fpr)\n",
    "print(\"tpr\")\n",
    "print(\"%.3f\" %tpr)\n",
    "print(\"***************************************************************\")\n",
    "\n",
    "\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(traindata, trainlabel)\n",
    "print(model)\n",
    "# make predictions\n",
    "expected = testlabel\n",
    "predicted = model.predict(testdata)\n",
    "np.savetxt('res/predictedDT.txt', predicted, fmt='%01d')\n",
    "# summarize the fit of the model\n",
    "accuracy = accuracy_score(expected, predicted)\n",
    "recall = recall_score(expected, predicted, average=\"binary\")\n",
    "precision = precision_score(expected, predicted , average=\"binary\")\n",
    "f1 = f1_score(expected, predicted , average=\"binary\")\n",
    "\n",
    "cm = metrics.confusion_matrix(expected, predicted)\n",
    "print(cm)\n",
    "tpr = float(cm[0][0])/np.sum(cm[0])\n",
    "fpr = float(cm[1][1])/np.sum(cm[1])\n",
    "print(\"%.3f\" %tpr)\n",
    "print(\"%.3f\" %fpr)\n",
    "print(\"Accuracy\")\n",
    "print(\"%.3f\" %accuracy)\n",
    "print(\"precision\")\n",
    "print(\"%.3f\" %precision)\n",
    "print(\"recall\")\n",
    "print(\"%.3f\" %recall)\n",
    "print(\"f-score\")\n",
    "print(\"%.3f\" %f1)\n",
    "print(\"fpr\")\n",
    "print(\"%.3f\" %fpr)\n",
    "print(\"tpr\")\n",
    "print(\"%.3f\" %tpr)\n",
    "print(\"***************************************************************\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = AdaBoostClassifier(n_estimators=100)\n",
    "model.fit(traindata, trainlabel)\n",
    "\n",
    "# make predictions\n",
    "expected = testlabel\n",
    "predicted = model.predict(testdata)\n",
    "np.savetxt('res/predictedABoost.txt', predicted, fmt='%01d')\n",
    "# summarize the fit of the model\n",
    "accuracy = accuracy_score(expected, predicted)\n",
    "recall = recall_score(expected, predicted, average=\"binary\")\n",
    "precision = precision_score(expected, predicted , average=\"binary\")\n",
    "f1 = f1_score(expected, predicted , average=\"binary\")\n",
    "\n",
    "cm = metrics.confusion_matrix(expected, predicted)\n",
    "print(cm)\n",
    "tpr = float(cm[0][0])/np.sum(cm[0])\n",
    "fpr = float(cm[1][1])/np.sum(cm[1])\n",
    "print(\"%.3f\" %tpr)\n",
    "print(\"%.3f\" %fpr)\n",
    "print(\"Accuracy\")\n",
    "print(\"%.3f\" %accuracy)\n",
    "print(\"precision\")\n",
    "print(\"%.3f\" %precision)\n",
    "print(\"recall\")\n",
    "print(\"%.3f\" %recall)\n",
    "print(\"f-score\")\n",
    "print(\"%.3f\" %f1)\n",
    "print(\"fpr\")\n",
    "print(\"%.3f\" %fpr)\n",
    "print(\"tpr\")\n",
    "print(\"%.3f\" %tpr)\n",
    "print(\"***************************************************************\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "model = model.fit(traindata, trainlabel)\n",
    "\n",
    "# make predictions\n",
    "expected = testlabel\n",
    "predicted = model.predict(testdata)\n",
    "np.savetxt('res/predictedRF.txt', predicted, fmt='%01d')\n",
    "# summarize the fit of the model\n",
    "accuracy = accuracy_score(expected, predicted)\n",
    "recall = recall_score(expected, predicted, average=\"binary\")\n",
    "precision = precision_score(expected, predicted , average=\"binary\")\n",
    "f1 = f1_score(expected, predicted , average=\"binary\")\n",
    "\n",
    "cm = metrics.confusion_matrix(expected, predicted)\n",
    "print(cm)\n",
    "tpr = float(cm[0][0])/np.sum(cm[0])\n",
    "fpr = float(cm[1][1])/np.sum(cm[1])\n",
    "print(\"%.3f\" %tpr)\n",
    "print(\"%.3f\" %fpr)\n",
    "print(\"Accuracy\")\n",
    "print(\"%.3f\" %accuracy)\n",
    "print(\"precision\")\n",
    "print(\"%.3f\" %precision)\n",
    "print(\"recall\")\n",
    "print(\"%.3f\" %recall)\n",
    "print(\"f-score\")\n",
    "print(\"%.3f\" %f1)\n",
    "print(\"fpr\")\n",
    "print(\"%.3f\" %fpr)\n",
    "print(\"tpr\")\n",
    "print(\"%.3f\" %tpr)\n",
    "print(\"***************************************************************\")\n",
    "\n",
    "\n",
    "\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "X_train = traindata\n",
    "y_train = trainlabel\n",
    "X_test = testdata\n",
    "y_test = testlabel\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "    clf = GridSearchCV(SVC(C=1), tuned_parameters, cv=5,\n",
    "                       scoring='%s_macro' % score)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print(\"----------------------------------------------\")\n",
    "    print(\"cross-validation accuracy of train data set\")\n",
    "    print(means)\n",
    "    \n",
    "    print(\"----------------------------------------------\")\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    #print(\"accuracy score\")\n",
    "    #print(accuracy_score(y_true, y_pred))\n",
    "    print(\"confusion matrix\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(\"Classification report\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()\n",
    "    print(\"***************************************************************************\")\n",
    "    print(\"for now\")\n",
    "    print(\"accuracy score\")\n",
    "    print(accuracy_score(y_true, y_pred))\n",
    "    print(\"precision\")\n",
    "    print(precision_score(y_true, y_pred , average=\"binary\"))\n",
    "    print(\"recall\")\n",
    "    print(recall_score(y_true, y_pred , average=\"binary\"))\n",
    "    print(\"F-score\")\n",
    "    print(f1_score(y_true, y_pred , average=\"binary\"))\n",
    "    print(\"best parameters\")\n",
    "    print(clf.best_params_)\n",
    "    print(\"***************************************************************************\")\n",
    "    predicted = y_pred\n",
    "    expected = y_true\n",
    "    cm = metrics.confusion_matrix(expected, predicted)\n",
    "    print(\"==============================================\")\n",
    "    print(cm)\n",
    "    tpr = float(cm[0][0])/np.sum(cm[0])\n",
    "    fpr = float(cm[1][1])/np.sum(cm[1])\n",
    "    print(\"%.3f\" %tpr)\n",
    "    print(\"%.3f\" %fpr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 20)\n",
    "kmeans\n",
    "\n",
    "KMmodel = kmeans.fit(trainData)\n",
    "KMmodel\n",
    "\n",
    "KMmodel.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
