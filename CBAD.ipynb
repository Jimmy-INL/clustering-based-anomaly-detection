{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Libraries for the Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the  Train Dataset and Checking if has missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = pd.read_csv(\"/Users/jeremyperez/Jupyter/NSL-KDD/KDDTrain+.csv\", header = None) \n",
    "#Run a Missing Value Ratio test to determine if any feature is missing values.\n",
    "#If all ratios = 0.0, then data is not missing any values for any features.\n",
    "\n",
    "#More info about Missing value ratio at \n",
    "#https://www.analyticsvidhya.com/blog/2018/08/dimensionality-reduction-techniques-python/\n",
    "\n",
    "trainData.isnull().sum()/len(trainData)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the  Test Dataset and Checking if has missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "testData = pd.read_csv(\"/Users/jeremyperez/Jupyter/NSL-KDD/KDDTest+.csv\", header = None)\n",
    "#Run a Missing Value Ratio test to determine if any feature is missing values.\n",
    "#If all ratios = 0.0, then data is not missing any values for any features.\n",
    "testData.isnull().sum()/len(testData)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the Dependent and independent Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = trainData.iloc[:,:-1].values # Get all the rows and all the clums except all the colums - 1\n",
    "Y = trainData.iloc[:,42].values# Get all the rows and the colum number 42\n",
    "A = testData.iloc[:,:-1].values # Get all the rows and all the clums except all the colums - 1\n",
    "Z = testData.iloc[:,42].values# Get all the rows and the colum number 42\n",
    "attacks = trainData.iloc[:,42].values #Attacks with no one hot encoding\n",
    "\n",
    "X = pd.DataFrame(X)\n",
    "Y = pd.DataFrame(Y)\n",
    "A = pd.DataFrame(A)\n",
    "Z = pd.DataFrame(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Categorical Data for Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "#We use One hot encoding to pervent the machine learning to atribute the categorical data in order. \n",
    "#What one hot encoding(ColumnTransformer) does is, it takes a column which has categorical data, \n",
    "#which has been label encoded, and then splits the column into multiple columns.\n",
    "#The numbers are replaced by 1s and 0s, depending on which column has what value\n",
    "#We don't need to do a label encoded step because ColumnTransformer do one hot encode and label encode!\n",
    "\n",
    "#Encoding the Independient Variable\n",
    "transformX = ColumnTransformer([(\"Servers\", OneHotEncoder(categories = \"auto\"), [1,2,3])], remainder=\"passthrough\")\n",
    "X = transformX.fit_transform(X)\n",
    "#Encoding the Dependent Variable\n",
    "transformY= ColumnTransformer([(\"Attacks\", OneHotEncoder(categories = \"auto\"), [0])], remainder=\"passthrough\")\n",
    "Y = transformY.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Categorical Data for Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding the Independient Variable\n",
    "transformA = ColumnTransformer([(\"Servers\", OneHotEncoder(categories = \"auto\"), [1,2,3])], remainder=\"passthrough\")\n",
    "A = transformA.fit_transform(A)\n",
    "    \n",
    "#Encoding the Dependent Variable\n",
    "transformZ = ColumnTransformer([(\"Attacks\", OneHotEncoder(categories = \"auto\"), [0])], remainder=\"passthrough\")\n",
    "Z = transformZ.fit_transform(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling the data with Standard Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Because we are using numerical-value-only clustering techniques to analyze the NSL-KDD dataset,\n",
    "#we need to normalize the values in the dataset, as Ibrahim., et. al. describe (page 112).\n",
    "#We complete the normalization process below:\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "trainScaler = StandardScaler()\n",
    "trainX = trainScaler.fit_transform(X)\n",
    "\n",
    "testScaler = StandardScaler()\n",
    "testA = testScaler.fit_transform(A)\n",
    "\n",
    "\n",
    "trainData = trainX #np.array(trainX)\n",
    "trainLabel = Y #np.array(Y)\n",
    "\n",
    "testData =  testA #np.array(testA)\n",
    "testLabel =  Z #np.array(Z)\n",
    "\n",
    "#model = LogisticRegression(solver = 'lbfgs')\n",
    "#model.fit(trainData,trainLabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elbow Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elbow method to find the best number of culster\n",
    "from sklearn.cluster import KMeans\n",
    "wcss = []\n",
    "for i in range(1,11):\n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++',max_iter = 300,n_init = 10,random_state = 0)\n",
    "    kmeans.fit(trainData)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(1,11),wcss)\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "#5 clusters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying K-mea(n_clusters = 5)\n",
    "KMEANS = KMeans(n_clusters = 4, init = 'k-means++',max_iter = 300,n_init = 10,random_state = 0)\n",
    "kmeans = KMEANS.fit(trainData)\n",
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Results by Crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(attacks,kmeans.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Results by Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visual representation of the clusters\n",
    "#plt.scatter(X[y_kmeans ==0,0],X[y_kmeans == 0,1], s = 21, c = 'red', label = 'cluster1')\n",
    "#plt.scatter(X[y_kmeans ==1,0],X[y_kmeans == 1,1], s = 21, c = 'yellow', label = 'cluster2')\n",
    "#plt.scatter(X[y_kmeans ==2,0],X[y_kmeans == 2,1], s = 21, c = 'cyan', label = 'cluster3')\n",
    "#plt.scatter(X[y_kmeans ==3,0],X[y_kmeans == 3,1], s = 21, c = 'orange', label = 'cluster4')\n",
    "#plt.scatter(X[y_kmeans ==4,0],X[y_kmeans == 4,1], s = 21, c = 'black', label = 'cluster5')\n",
    "#plt.scatter(kmeans.cluster_centers_[:, 0],kmeans.cluster_centers_[:, 1],s = 300, c = 'purple', label = 'Centroids')\n",
    "#plt.title('Clusters of Attacks')\n",
    "#plt.xlabel('Numbers of Attacks')\n",
    "#plt.ylabel('Types of Attacks')\n",
    "#plt.legend()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import accuracy_score\n",
    "#OPTICS\n",
    "# #############################################################################\n",
    "# Compute DBSCAN\n",
    "db = DBSCAN(eps=0.3, min_samples=10).fit(trainData)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "pd.crosstab(attacks,labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F-Score implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "fscore = f1_score(X,Y).fit(X)\n",
    "print(fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "923px",
    "left": "328px",
    "right": "20px",
    "top": "9px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
