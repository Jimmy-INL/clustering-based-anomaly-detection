{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the Data Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readingData(path):\n",
    "    \n",
    "    #Reading the Train Dataset\n",
    "    \n",
    "    #trainData = pd.read_csv(\"/Users/bethanydanner/Google_Drive/documents/python_code/clustering-based-anomaly-detection/Dataset/NSL-KDD/KDDTrain+.csv\", header = None)\n",
    "    dataSet = pd.read_csv(path, header = None)\n",
    "    \n",
    "    return dataSet\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "#trainData = pd.read_csv(\"/Users/bethanydanner/Google_Drive/documents/python_code/clustering-based-anomaly-detection/Dataset/NSL-KDD/KDDTrain+.csv\", header = None)\n",
    "dataSet = readingData(\"/Users/jeremyperez/Jupyter/NSL-KDD/KDDTrain+.csv\")\n",
    "\n",
    "#Run a Missing Value Ratio test to determine if any feature is missing values.\n",
    "#If all ratios = 0.0, then data is not missing any values for any features.\n",
    "dataSet.isnull().sum()/len(dataSet)*100\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting The data we want to test for the clustering algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gettingVariables(dataSet):\n",
    "    #Getting the Dependent and independent Variables\n",
    "    X = dataSet.iloc[:,:-2].values # Data, Get all the rows and all the clums except all the colums - 2\n",
    "    Y = dataSet.iloc[:,42].values# Labels\n",
    "\n",
    "    #Removing Categorical data from the data set\n",
    "    Z = dataSet.iloc[:,[0,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40]].values\n",
    "    \n",
    "    #Removing server types\n",
    "    W = dataSet.iloc[:,[0,1,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40]].values\n",
    "    \n",
    "    #Removing Protocols to start using risk Values\n",
    "    R = dataSet.iloc[:,[0,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40]].values\n",
    "\n",
    "    return X,Y,Z,W,R\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "data,labels,noCatg,noServ,riskVal  = gettingVariables(dataSet) #Getting the Data we want to use for the algorithms\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing the attacks into binary lables normal and abnormal\n",
    "# Encoding binary labels into 0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodingLabels(labels):\n",
    "    #Attacks\n",
    "    \n",
    "    #Binary Categories\n",
    "    attackType  = {'normal': \"normal\", 'neptune': \"abnormal\", 'warezclient': \"abnormal\", 'ipsweep': \"abnormal\",'back': \"abnormal\", 'smurf': \"abnormal\", 'rootkit': \"abnormal\",'satan': \"abnormal\", 'guess_passwd': \"abnormal\",'portsweep': \"abnormal\",'teardrop': \"abnormal\",'nmap': \"abnormal\",'pod': \"abnormal\",'ftp_write': \"abnormal\",'multihop': \"abnormal\",'buffer_overflow': \"abnormal\",'imap': \"abnormal\",'warezmaster': \"abnormal\",'phf': \"abnormal\",'land': \"abnormal\",'loadmodule': \"abnormal\",'spy': \"abnormal\",'perl': \"abnormal\"} \n",
    "    attackEncodingCluster  = {'normal': 0,'abnormal': 1}\n",
    "    \n",
    "    labels[:] = [attackEncodingCluster[item] for item in labels[:]]#Changing the names of the labels to binary labels normal and abnormal\n",
    "    labels[:] = [attackType[item] for item in labels[:]] #Encoding the binary data\n",
    "\n",
    "    #4 Main Categories\n",
    "    #attackType  = {'normal': \"normal\", 'neptune': \"DoS\", 'warezclient': \"R2L\", 'ipsweep': \"Probe\",'back': \"DoS\", 'smurf': \"DoS\", 'rootkit': \"U2R\",'satan': \"Probe\", 'guess_passwd': \"R2L\",'portsweep': \"Probe\",'teardrop': \"DoS\",'nmap': \"Probe\",'pod': \"DoS\",'ftp_write': \"R2L\",'multihop': \"R2L\",'buffer_overflow': \"U2R\",'imap': \"R2L\",'warezmaster': \"R2L\",'phf': \"R2L\",'land': \"DoS\",'loadmodule': \"U2R\",'spy': \"R2L\",'perl': \"U2R\"} \n",
    "    #attackEncodingCluster  = {'normal': 0,'DoS': 1,'Probe': 2,'R2L': 3, 'U2R': 4} #Main Categories\n",
    "    \n",
    "    #labels[:] = [attackEncodingCluster[item] for item in labels[:]]# Changing the names of attacks into 4 main categories\n",
    "    #labels[:] = [attackType[item] for item in labels[:]] #Encoding the main 4 categories\n",
    "    \n",
    "    #normal = 0\n",
    "    #DoS = 1\n",
    "    #Probe = 2\n",
    "    #R2L = 3\n",
    "    #U2R = 4\n",
    "    \n",
    "    return labels\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "labels = encodingLabels(labels)\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding the data using one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotEncodingData(data): \n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    \n",
    "    #We use One hot encoding to pervent the machine learning to atribute the categorical data in order. \n",
    "    #What one hot encoding(ColumnTransformer) does is, it takes a column which has categorical data, \n",
    "    #which has been label encoded, and then splits the column into multiple columns.\n",
    "    #The numbers are replaced by 1s and 0s, depending on which column has what value\n",
    "    #We don't need to do a label encoded step because ColumnTransformer do one hot encode and label encode!\n",
    "\n",
    "    #Encoding the Independient Variable\n",
    "    transform = ColumnTransformer([(\"Servers\", OneHotEncoder(categories = \"auto\"), [1,2,3])], remainder=\"passthrough\")\n",
    "    data = transform.fit_transform(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "data = oneHotEncodingData(data) #One hot Encode with the complete data\n",
    "#noServ = encodingData(noServ) #One hot Encode with no Server Type\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Risk Encoding Data\n",
    "# Using risk values to encode the data with those values (only for risk test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def riskEncodingData(data,labels): \n",
    "    #Manually Encoding for the attacks types only\n",
    "    data = pd.DataFrame(data)\n",
    "    servers  = {'http': 0.01, 'domain_u': 0, 'sunrpc': 1, 'smtp': 0.01, 'ecr_i': 0.87, 'iso_tsap': 1, 'private': 0.97, 'finger': 0.27, 'ftp': 0.26, 'telnet': 0.48,'other': 0.12,'discard': 1, 'courier': 1, 'pop_3': 0.53, 'ldap': 1, 'eco_i': 0.8, 'ftp_data': 0.06, 'klogin': 1, 'auth': 0.31, 'mtp': 1, 'name': 1, 'netbios_ns': 1,'remote_job': 1,'supdup': 1,'uucp_path': 1,'Z39_50': 1,'csnet_ns': 1,'uucp': 1,'netbios_dgm': 1,'urp_i': 0,'domain': 0.96,'bgp':1,'gopher': 1,'vmnet': 1,'systat': 1,'http_443': 1,'efs': 1,'whois': 1,'imap4': 1,'echo': 1,'link': 1,'login': 1,'kshell': 1,'sql_net': 1,'time': 0.88,'hostnames': 1,'exec': 1,'ntp_u': 0,'nntp': 1,'ctf': 1,'ssh': 1,'daytime': 1,'shell': 1,'netstat': 1,'nnsp': 1,'IRC': 0,'pop_2': 1,'printer': 1,'tim_i': 0.33,'pm_dump': 1,'red_i': 0,'netbios_ssn': 1,'rje': 1,'X11': 0.04,'urh_i': 0,'http_8001': 1,'aol': 1,'http_2784': 1,'tftp_u': 0,'harvest': 1}\n",
    "    data[1] = [servers[item] for item in data[1]]\n",
    "\n",
    "    servers_error  = {'REJ': 0.519, 'SF': 0.016, 'S0': 0.998, 'RSTR': 0.882, 'RSTO': 0.886,'SH': 0.993,'S1': 0.008,'RSTOS0': 1,'S3': 0.08,'S2': 0.05,'OTH': 0.729} \n",
    "    data[2] = [servers_error[item] for item in data[2]]\n",
    "\n",
    "    #Attacks\n",
    "    attackType  = {'normal': \"normal\", 'neptune': \"abnormal\", 'warezclient': \"abnormal\", 'ipsweep': \"abnormal\",'back': \"abnormal\", 'smurf': \"abnormal\", 'rootkit': \"abnormal\",'satan': \"abnormal\", 'guess_passwd': \"abnormal\",'portsweep': \"abnormal\",'teardrop': \"abnormal\",'nmap': \"abnormal\",'pod': \"abnormal\",'ftp_write': \"abnormal\",'multihop': \"abnormal\",'buffer_overflow': \"abnormal\",'imap': \"abnormal\",'warezmaster': \"abnormal\",'phf': \"abnormal\",'land': \"abnormal\",'loadmodule': \"abnormal\",'spy': \"abnormal\",'perl': \"abnormal\"} \n",
    "    #attackType  = {'normal': \"normal\", 'neptune': \"DoS\", 'warezclient': \"R2L\", 'ipsweep': \"Probe\",'back': \"DoS\", 'smurf': \"DoS\", 'rootkit': \"U2R\",'satan': \"Probe\", 'guess_passwd': \"R2L\",'portsweep': \"Probe\",'teardrop': \"DoS\",'nmap': \"Probe\",'pod': \"DoS\",'ftp_write': \"R2L\",'multihop': \"R2L\",'buffer_overflow': \"U2R\",'imap': \"R2L\",'warezmaster': \"R2L\",'phf': \"R2L\",'land': \"DoS\",'loadmodule': \"U2R\",'spy': \"R2L\",'perl': \"U2R\"} \n",
    "    labels[:] = [attackType[item] for item in labels[:]]\n",
    "    \n",
    "    #attackEncodingCluster  = {'normal': 0,'DoS': 1,'Probe': 2,'R2L': 3, 'U2R': 4} #Main Categories\n",
    "    attackEncodingCluster  = {'normal': 0,'abnormal': 1}  #Binary Categories\n",
    "    labels[:] = [attackEncodingCluster[item] for item in labels[:]]\n",
    "    \n",
    "    return data,labels\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "riskVal,labels = riskEncodingData(riskVal,labels)\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalign the data with the normalize method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizing(data):\n",
    "    \n",
    "    from sklearn.preprocessing import Normalizer\n",
    "    #Because we are using numerical-value-only clustering techniques to analyze the NSL-KDD dataset,\n",
    "    #we need to normalize the values in the dataset, as Ibrahim., et. al. describe (page 112).\n",
    "    #Normalize works by scaling the features in a range of [0,1]\n",
    "    #We complete the normalization process below:\n",
    "    normalizer = Normalizer().fit(data)\n",
    "    data = normalizer.transform(data)\n",
    "    data = pd.DataFrame(data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "#data = normalizing(data) #CategoricalData\n",
    "noCatg = normalizing(noCatg) #No categorical data\n",
    "#noServ = normalizing(noServ) #No Server Type\n",
    "#riskVal = normalizing(riskVal) #Risk values with no protocols colum\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMEANS Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeansClustering(data): \n",
    "    from sklearn.cluster import KMeans\n",
    "    \n",
    "    KMEANS = KMeans(n_clusters = 5, init = 'k-means++',max_iter = 300,n_init = 10,random_state = 0)\n",
    "    kmeans = KMEANS.fit(data)\n",
    "    klabels = kmeans.labels_\n",
    "    return klabels\n",
    "\n",
    "#########################################################################\n",
    "#KMEANS\n",
    "klabels = kmeansClustering(data) #Categorical data Kmeans Algorithm\n",
    "#klabels = kmeansClustering(noCatg) #No Categorical Data, Kmeans Algorithm\n",
    "#klabels = kmeansClustering(noServ) #No server Type Data, Kmeans Algorithm\n",
    "#klabels = kmeansClustering(riskVal) #Risk values with no protocols colum Data, Kmeans Algorithm\n",
    "\n",
    "\n",
    "#Kmeans Results\n",
    "kmeansR = pd.crosstab(labels,klabels)\n",
    "kmeansR.idxmax()\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1-Score for Kmeans algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kF1(klabels,labels): #F1 Score for Kmeans\n",
    "    from sklearn.metrics import f1_score\n",
    "\n",
    "    attackEncodingCluster  = {0: 0, 1: 1, 2: 1, 3: 1, 4: 1}\n",
    "    klabels[:] = [attackEncodingCluster[item] for item in klabels[:]]\n",
    "    \n",
    "    labels = np.array(labels,dtype = int)\n",
    "    f1 = f1_score(labels,klabels, average = 'weighted') #[None, 'micro', 'macro', 'weighted']\n",
    "    print(f1)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "#########################################################################\n",
    "#F1 Score kmeans\n",
    "kmeansF1 = kF1(klabels,labels)\n",
    "kmeansF1\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscanClustering(data): #DBSCAN algorithm\n",
    "    from sklearn.cluster import DBSCAN\n",
    "    \n",
    "    #Compute DBSCAN\n",
    "    db = DBSCAN(eps=0.7, min_samples = 40000).fit(data)\n",
    "    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "    core_samples_mask[db.core_sample_indices_] = True\n",
    "    dblabels = db.labels_\n",
    "    # Number of clusters in labels, ignoring noise if present.\n",
    "    n_clusters_ = len(set(dblabels)) - (1 if -1 in dblabels else 0)\n",
    "    n_noise_ = list(dblabels).count(-1)\n",
    "    return dblabels,n_clusters_,n_noise_\n",
    "#########################################################################\n",
    "#DBSCAN\n",
    "#dblabels = dbscanClustering(data) #Categorical Data DBSCAN Algorithm\n",
    "dblabels,nClusters,nNoises = dbscanClustering(noCatg) #No Categorical Data, DBSCAN Algorithm\n",
    "#dblabels,nClusters,nNoises = dbscanClustering(noServ) #No Server Type Data, DBSCAN Algorithm\n",
    "#dblabels,nClusters,nNoises = dbscanClustering(riskVal) #Risk values with no protocols colum Data,DBSCAN Algorithm\n",
    "\n",
    "\n",
    "#DBSCAN Results\n",
    "dbscanR = pd.crosstab(labels,dblabels)\n",
    "dbscanR.idxmax()\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1-Score for DBSCAN algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbF1(dblabels,labels):\n",
    "    from sklearn.metrics import f1_score\n",
    "    #Encoding data to F-score\n",
    "    #normal = 0\n",
    "    #DoS = 1\n",
    "    #Probe = 2\n",
    "    #R2L = 3\n",
    "    #U2R = 4\n",
    "    attackEncodingCluster  = {-1: 1, 0: 0, 1: 0, 2: 0, 3: 0, 4: 1, 5: 0, 6: 1,7:1}\n",
    "    dblabels[:] = [attackEncodingCluster[item] for item in dblabels[:]]\n",
    "    \n",
    "    labels = np.array(labels,dtype = int)\n",
    "    f1 = f1_score(labels,dblabels, average = 'weighted') #[None, 'micro', 'macro', 'weighted']\n",
    "    \n",
    "    return f1\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "#F1 Score dbscan\n",
    "dbscanF1 = dbF1(dblabels,labels)\n",
    "dbscanF1\n",
    "#########################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "923px",
    "left": "328px",
    "right": "20px",
    "top": "9px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
